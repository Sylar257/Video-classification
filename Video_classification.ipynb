{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commonly used CV tools\n",
    "import os\n",
    "import cv2     # for capturing videos\n",
    "import math   # for mathematical operations\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt    # for plotting the images\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np    # for mathematical operations\n",
    "from skimage.transform import resize   # for resizing images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for model architectures\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file names into the train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      video_name\n",
       "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
       "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
       "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
       "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
       "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the .txt file which have names of training videos\n",
    "f = open(\"trainlist01.txt\", \"r\")\n",
    "temp = f.read()\n",
    "videos = temp.split('\\n')\n",
    "\n",
    "# creating a dataframe having video names\n",
    "train = pd.DataFrame()\n",
    "train['video_name'] = videos\n",
    "train = train[:-1]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the tagnames from folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      video_name             tag\n",
       "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1  ApplyEyeMakeup\n",
       "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1  ApplyEyeMakeup\n",
       "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1  ApplyEyeMakeup\n",
       "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1  ApplyEyeMakeup\n",
       "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1  ApplyEyeMakeup"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_video_tag = []\n",
    "for i in range(train.shape[0]):\n",
    "    train_video_tag.append(train['video_name'][i].split('/')[0])\n",
    "\n",
    "train['tag'] = train_video_tag\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test data frame and corresponding tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    video_name             tag\n",
       "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi  ApplyEyeMakeup\n",
       "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi  ApplyEyeMakeup\n",
       "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi  ApplyEyeMakeup\n",
       "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi  ApplyEyeMakeup\n",
       "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi  ApplyEyeMakeup"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the .txt file which have names of test videos\n",
    "f = open(\"testlist01.txt\", \"r\")\n",
    "temp = f.read()\n",
    "videos = temp.split('\\n')\n",
    "\n",
    "# creating a dataframe having video names\n",
    "test = pd.DataFrame()\n",
    "test['video_name'] = videos\n",
    "test = test[:-1]\n",
    "\n",
    "# creating tags for test videos\n",
    "test_video_tag = []\n",
    "for i in range(test.shape[0]):\n",
    "    test_video_tag.append(test['video_name'][i].split('/')[0])\n",
    "    \n",
    "test['tag'] = test_video_tag\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new folder 'train_1' to contain extracted frames\n",
    "use `cap.get( )` from `cv2` to get certain properties of the video capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9537/9537 [06:39<00:00, 23.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# storing the frames from training videos\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    count = 0\n",
    "    videoFile = train['video_name'][i]\n",
    "    cap = cv2.VideoCapture('UCF-101/'+videoFile.split(' ')[0])\n",
    "    frameRate = cap.get(5) # get frames per second\n",
    "    print(f'The video is taking at {frameRate} frames per second')\n",
    "    \n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(1) # get current frame number\n",
    "        ret, frame = cap.read()\n",
    "        if(ret != True):\n",
    "            break\n",
    "        if (frameId % math.floor(frameRate) == 0):\n",
    "            # storing the frames in a new folder named train_1\n",
    "            filename = 'train_1/' + videoFile.split('/')[1].split(' ')[0] +\"_frame%d.jpg\" % count;count+=1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, create a `.csv` file that contains paths to these images as well as their `class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73844/73844 [00:00<00:00, 600840.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the names of all the images\n",
    "images = glob('train_1/*.jpg')\n",
    "train_image = []\n",
    "train_class = []\n",
    "for i in tqdm(range(len(images))):\n",
    "    # create the image name\n",
    "    train_image.append(images[i].split('/')[1])\n",
    "    # create the class of this image, the activity name\n",
    "    train_class.append(images[i].split('/')[1].split('_')[1])\n",
    "    \n",
    "# storing the images and their class in a dataframe\n",
    "train_data = pd.DataFrame()\n",
    "train_data['image'] = train_image\n",
    "train_data['class'] = train_class\n",
    "\n",
    "# save dataframe into `.csv` file\n",
    "train_data.to_csv('UCF-101/train_new.csv', header = True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training most basic video classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we will consider using the most basic architecure 3D-CNN with a very light base architecture: VGG-16\n",
    "We have created our training image names are corresponding classes in a dataframe.\n",
    "Now we just need to:\n",
    "* Define model architecture\n",
    "* Train and validate performance using unseen data\n",
    "* Hyper-parameter tuning\n",
    "* Upgrade model capability and repeat process for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './jpegs_256'                 # preprocessed RGB images\n",
    "action_name_path = './UCF101actions.pkl' # preprocessed action names\n",
    "save_model_path = './Conv3D_ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D CNN patameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture parameters\n",
    "fc_hidden1, fc_hidden2 = 256, 256\n",
    "dropout = 0.0        # dropout probability\n",
    "\n",
    "# training parameters\n",
    "k = 101            # number of target category\n",
    "epochs = 10        # do adjust here for shorter training period\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "log_interval = 10\n",
    "img_x, img_y = 256, 342  # resize video 2d frame size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For 3D CNN we will need to determine a fixed No.frames and here we take 28 for UCF 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_frame, end_frame, skip_frame = 1, 29, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train() # training model, enable dropout and \n",
    "    \n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0 # count total trained sample in one epoch\n",
    "    \n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device).view(-1,)\n",
    "        \n",
    "        N_count += X.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)    # output size = (batch, number of classes)\n",
    "        \n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy()) # computed on CPU\n",
    "        scores.append(step_score)         \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # to compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(model.state_dict(), os.path.join(save_model_path, '3dcnn_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, '3dcnn_optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 101 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "# load UCF101 label categories\n",
    "with open(action_name_path, 'rb') as f:\n",
    "    action_names = pickle.load(f)\n",
    "    \n",
    "print(f'we have {len(action_names)} categories')\n",
    "\n",
    "# conver labels to categories using LabelEncoder()\n",
    "le = LabelEncoder()\n",
    "le.fit(action_names)\n",
    "\n",
    "# Then one-hot-encoding the categorical labels\n",
    "action_category = le.transform(action_names).reshape(-1,1)  # map from names to a number from [0,100]\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(action_category)\n",
    "\n",
    "actions = []\n",
    "fnames = os.listdir(data_path)\n",
    "\n",
    "all_names = []\n",
    "for f in fnames:\n",
    "    loc1 = f.find('v_')\n",
    "    loc2 = f.find('_g')\n",
    "    actions.append(f[(loc1 + 2): loc2])\n",
    "\n",
    "    all_names.append(f)\n",
    "\n",
    "all_X_list = all_names\n",
    "all_y_list = labels2cat(le, actions)\n",
    "\n",
    "# train, test split\n",
    "train_list, test_list, train_label, test_label = train_test_split(all_X_list, all_y_list, test_size=0.25, random_state=42)\n",
    "\n",
    "# image transformation\n",
    "transform = transforms.Compose([transforms.Resize([img_x, img_y]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist() # a list from 1 to 28\n",
    "\n",
    "train_set = Dataset_3DCNN(data_path, train_list, train_label, selected_frames, transform=transform)\n",
    "valid_set = Dataset_3DCNN(data_path, test_list, test_list,    selected_frames, transform=transform) \n",
    "\n",
    "# adjust params from above\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)\n",
    "\n",
    "cnn3d = CNN3D(t_dim=len(selected_frames), \n",
    "              img_x=img_x, \n",
    "              img_y=img_y, \n",
    "              drop_p=dropout, \n",
    "              fc_hidden1=fc_hidden1,\n",
    "              fc_hidden2=fc_hidden2,\n",
    "              num_classes=k\n",
    "             ).to(device)\n",
    "\n",
    "# use classic adam optimizer\n",
    "optimizer = torch.optim.Adam(cnn3d.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [80/9990 (1%)]\tLoss: 9.080015, Accu: 0.00%\n",
      "Train Epoch: 1 [160/9990 (2%)]\tLoss: 5.893954, Accu: 0.00%\n",
      "Train Epoch: 1 [240/9990 (2%)]\tLoss: 5.528685, Accu: 0.00%\n",
      "Train Epoch: 1 [320/9990 (3%)]\tLoss: 6.027668, Accu: 0.00%\n",
      "Train Epoch: 1 [400/9990 (4%)]\tLoss: 4.651013, Accu: 0.00%\n",
      "Train Epoch: 1 [480/9990 (5%)]\tLoss: 5.950002, Accu: 0.00%\n",
      "Train Epoch: 1 [560/9990 (6%)]\tLoss: 4.903268, Accu: 0.00%\n",
      "Train Epoch: 1 [640/9990 (6%)]\tLoss: 4.571974, Accu: 0.00%\n",
      "Train Epoch: 1 [720/9990 (7%)]\tLoss: 4.747395, Accu: 0.00%\n",
      "Train Epoch: 1 [800/9990 (8%)]\tLoss: 4.708570, Accu: 0.00%\n",
      "Train Epoch: 1 [880/9990 (9%)]\tLoss: 4.786088, Accu: 0.00%\n",
      "Train Epoch: 1 [960/9990 (10%)]\tLoss: 4.351201, Accu: 12.50%\n",
      "Train Epoch: 1 [1040/9990 (10%)]\tLoss: 4.680658, Accu: 0.00%\n",
      "Train Epoch: 1 [1120/9990 (11%)]\tLoss: 4.626257, Accu: 0.00%\n",
      "Train Epoch: 1 [1200/9990 (12%)]\tLoss: 4.620758, Accu: 0.00%\n",
      "Train Epoch: 1 [1280/9990 (13%)]\tLoss: 4.611176, Accu: 0.00%\n",
      "Train Epoch: 1 [1360/9990 (14%)]\tLoss: 4.625695, Accu: 12.50%\n",
      "Train Epoch: 1 [1440/9990 (14%)]\tLoss: 4.600945, Accu: 0.00%\n",
      "Train Epoch: 1 [1520/9990 (15%)]\tLoss: 4.639880, Accu: 0.00%\n",
      "Train Epoch: 1 [1600/9990 (16%)]\tLoss: 4.203201, Accu: 12.50%\n",
      "Train Epoch: 1 [1680/9990 (17%)]\tLoss: 4.453645, Accu: 12.50%\n",
      "Train Epoch: 1 [1760/9990 (18%)]\tLoss: 4.629068, Accu: 0.00%\n",
      "Train Epoch: 1 [1840/9990 (18%)]\tLoss: 4.623846, Accu: 0.00%\n",
      "Train Epoch: 1 [1920/9990 (19%)]\tLoss: 4.598883, Accu: 0.00%\n",
      "Train Epoch: 1 [2000/9990 (20%)]\tLoss: 4.624166, Accu: 0.00%\n",
      "Train Epoch: 1 [2080/9990 (21%)]\tLoss: 4.602293, Accu: 0.00%\n",
      "Train Epoch: 1 [2160/9990 (22%)]\tLoss: 4.647404, Accu: 0.00%\n",
      "Train Epoch: 1 [2240/9990 (22%)]\tLoss: 4.541629, Accu: 0.00%\n",
      "Train Epoch: 1 [2320/9990 (23%)]\tLoss: 4.609963, Accu: 0.00%\n",
      "Train Epoch: 1 [2400/9990 (24%)]\tLoss: 4.575854, Accu: 0.00%\n",
      "Train Epoch: 1 [2480/9990 (25%)]\tLoss: 4.627385, Accu: 0.00%\n",
      "Train Epoch: 1 [2560/9990 (26%)]\tLoss: 4.629862, Accu: 0.00%\n",
      "Train Epoch: 1 [2640/9990 (26%)]\tLoss: 4.616962, Accu: 0.00%\n",
      "Train Epoch: 1 [2720/9990 (27%)]\tLoss: 4.537766, Accu: 0.00%\n",
      "Train Epoch: 1 [2800/9990 (28%)]\tLoss: 4.611341, Accu: 0.00%\n",
      "Train Epoch: 1 [2880/9990 (29%)]\tLoss: 4.601112, Accu: 0.00%\n",
      "Train Epoch: 1 [2960/9990 (30%)]\tLoss: 4.580690, Accu: 12.50%\n",
      "Train Epoch: 1 [3040/9990 (30%)]\tLoss: 4.592881, Accu: 12.50%\n",
      "Train Epoch: 1 [3120/9990 (31%)]\tLoss: 4.617578, Accu: 0.00%\n",
      "Train Epoch: 1 [3200/9990 (32%)]\tLoss: 4.588271, Accu: 0.00%\n",
      "Train Epoch: 1 [3280/9990 (33%)]\tLoss: 4.539935, Accu: 0.00%\n",
      "Train Epoch: 1 [3360/9990 (34%)]\tLoss: 4.632539, Accu: 0.00%\n",
      "Train Epoch: 1 [3440/9990 (34%)]\tLoss: 4.616118, Accu: 0.00%\n",
      "Train Epoch: 1 [3520/9990 (35%)]\tLoss: 4.570795, Accu: 0.00%\n",
      "Train Epoch: 1 [3600/9990 (36%)]\tLoss: 4.530873, Accu: 0.00%\n",
      "Train Epoch: 1 [3680/9990 (37%)]\tLoss: 4.585591, Accu: 12.50%\n",
      "Train Epoch: 1 [3760/9990 (38%)]\tLoss: 4.046960, Accu: 12.50%\n",
      "Train Epoch: 1 [3840/9990 (38%)]\tLoss: 4.631300, Accu: 0.00%\n",
      "Train Epoch: 1 [3920/9990 (39%)]\tLoss: 4.049702, Accu: 12.50%\n",
      "Train Epoch: 1 [4000/9990 (40%)]\tLoss: 4.596971, Accu: 12.50%\n",
      "Train Epoch: 1 [4080/9990 (41%)]\tLoss: 4.587598, Accu: 0.00%\n",
      "Train Epoch: 1 [4160/9990 (42%)]\tLoss: 4.702766, Accu: 0.00%\n",
      "Train Epoch: 1 [4240/9990 (42%)]\tLoss: 4.593814, Accu: 12.50%\n",
      "Train Epoch: 1 [4320/9990 (43%)]\tLoss: 4.633808, Accu: 0.00%\n",
      "Train Epoch: 1 [4400/9990 (44%)]\tLoss: 4.564917, Accu: 0.00%\n",
      "Train Epoch: 1 [4480/9990 (45%)]\tLoss: 4.689115, Accu: 0.00%\n",
      "Train Epoch: 1 [4560/9990 (46%)]\tLoss: 4.605130, Accu: 0.00%\n",
      "Train Epoch: 1 [4640/9990 (46%)]\tLoss: 4.612843, Accu: 0.00%\n",
      "Train Epoch: 1 [4720/9990 (47%)]\tLoss: 4.760650, Accu: 0.00%\n",
      "Train Epoch: 1 [4800/9990 (48%)]\tLoss: 4.593365, Accu: 0.00%\n",
      "Train Epoch: 1 [4880/9990 (49%)]\tLoss: 4.613650, Accu: 0.00%\n",
      "Train Epoch: 1 [4960/9990 (50%)]\tLoss: 4.610714, Accu: 0.00%\n",
      "Train Epoch: 1 [5040/9990 (50%)]\tLoss: 4.599548, Accu: 12.50%\n",
      "Train Epoch: 1 [5120/9990 (51%)]\tLoss: 4.625787, Accu: 0.00%\n",
      "Train Epoch: 1 [5200/9990 (52%)]\tLoss: 4.632189, Accu: 0.00%\n",
      "Train Epoch: 1 [5280/9990 (53%)]\tLoss: 4.515827, Accu: 12.50%\n",
      "Train Epoch: 1 [5360/9990 (54%)]\tLoss: 4.598088, Accu: 12.50%\n",
      "Train Epoch: 1 [5440/9990 (54%)]\tLoss: 4.609584, Accu: 0.00%\n",
      "Train Epoch: 1 [5520/9990 (55%)]\tLoss: 5.267601, Accu: 0.00%\n",
      "Train Epoch: 1 [5600/9990 (56%)]\tLoss: 4.892030, Accu: 0.00%\n",
      "Train Epoch: 1 [5680/9990 (57%)]\tLoss: 4.606448, Accu: 0.00%\n",
      "Train Epoch: 1 [5760/9990 (58%)]\tLoss: 4.591936, Accu: 0.00%\n",
      "Train Epoch: 1 [5840/9990 (58%)]\tLoss: 4.632029, Accu: 0.00%\n",
      "Train Epoch: 1 [5920/9990 (59%)]\tLoss: 4.638648, Accu: 0.00%\n",
      "Train Epoch: 1 [6000/9990 (60%)]\tLoss: 4.419276, Accu: 12.50%\n",
      "Train Epoch: 1 [6080/9990 (61%)]\tLoss: 4.699730, Accu: 0.00%\n",
      "Train Epoch: 1 [6160/9990 (62%)]\tLoss: 4.620680, Accu: 0.00%\n",
      "Train Epoch: 1 [6240/9990 (62%)]\tLoss: 4.592740, Accu: 0.00%\n",
      "Train Epoch: 1 [6320/9990 (63%)]\tLoss: 4.594992, Accu: 0.00%\n",
      "Train Epoch: 1 [6400/9990 (64%)]\tLoss: 4.546056, Accu: 12.50%\n",
      "Train Epoch: 1 [6480/9990 (65%)]\tLoss: 4.634822, Accu: 12.50%\n",
      "Train Epoch: 1 [6560/9990 (66%)]\tLoss: 4.612893, Accu: 0.00%\n",
      "Train Epoch: 1 [6640/9990 (66%)]\tLoss: 4.608079, Accu: 12.50%\n",
      "Train Epoch: 1 [6720/9990 (67%)]\tLoss: 4.641359, Accu: 0.00%\n",
      "Train Epoch: 1 [6800/9990 (68%)]\tLoss: 4.621462, Accu: 0.00%\n",
      "Train Epoch: 1 [6880/9990 (69%)]\tLoss: 4.451395, Accu: 12.50%\n",
      "Train Epoch: 1 [6960/9990 (70%)]\tLoss: 4.621053, Accu: 12.50%\n",
      "Train Epoch: 1 [7040/9990 (70%)]\tLoss: 4.723307, Accu: 0.00%\n",
      "Train Epoch: 1 [7120/9990 (71%)]\tLoss: 4.817513, Accu: 0.00%\n",
      "Train Epoch: 1 [7200/9990 (72%)]\tLoss: 4.600076, Accu: 0.00%\n",
      "Train Epoch: 1 [7280/9990 (73%)]\tLoss: 4.604130, Accu: 0.00%\n",
      "Train Epoch: 1 [7360/9990 (74%)]\tLoss: 4.560793, Accu: 0.00%\n",
      "Train Epoch: 1 [7440/9990 (74%)]\tLoss: 4.489141, Accu: 0.00%\n",
      "Train Epoch: 1 [7520/9990 (75%)]\tLoss: 4.526449, Accu: 12.50%\n",
      "Train Epoch: 1 [7600/9990 (76%)]\tLoss: 4.628278, Accu: 0.00%\n",
      "Train Epoch: 1 [7680/9990 (77%)]\tLoss: 4.527750, Accu: 0.00%\n",
      "Train Epoch: 1 [7760/9990 (78%)]\tLoss: 4.504481, Accu: 0.00%\n",
      "Train Epoch: 1 [7840/9990 (78%)]\tLoss: 4.601497, Accu: 0.00%\n",
      "Train Epoch: 1 [7920/9990 (79%)]\tLoss: 4.478760, Accu: 0.00%\n",
      "Train Epoch: 1 [8000/9990 (80%)]\tLoss: 4.598686, Accu: 0.00%\n",
      "Train Epoch: 1 [8080/9990 (81%)]\tLoss: 4.601302, Accu: 0.00%\n",
      "Train Epoch: 1 [8160/9990 (82%)]\tLoss: 4.620996, Accu: 0.00%\n",
      "Train Epoch: 1 [8240/9990 (82%)]\tLoss: 4.590595, Accu: 0.00%\n",
      "Train Epoch: 1 [8320/9990 (83%)]\tLoss: 4.422671, Accu: 0.00%\n",
      "Train Epoch: 1 [8400/9990 (84%)]\tLoss: 4.650793, Accu: 0.00%\n",
      "Train Epoch: 1 [8480/9990 (85%)]\tLoss: 4.754405, Accu: 0.00%\n",
      "Train Epoch: 1 [8560/9990 (86%)]\tLoss: 4.641786, Accu: 0.00%\n",
      "Train Epoch: 1 [8640/9990 (86%)]\tLoss: 4.686349, Accu: 0.00%\n",
      "Train Epoch: 1 [8720/9990 (87%)]\tLoss: 4.513115, Accu: 0.00%\n",
      "Train Epoch: 1 [8800/9990 (88%)]\tLoss: 4.550436, Accu: 0.00%\n",
      "Train Epoch: 1 [8880/9990 (89%)]\tLoss: 4.613772, Accu: 0.00%\n",
      "Train Epoch: 1 [8960/9990 (90%)]\tLoss: 4.568275, Accu: 0.00%\n",
      "Train Epoch: 1 [9040/9990 (90%)]\tLoss: 4.444973, Accu: 0.00%\n",
      "Train Epoch: 1 [9120/9990 (91%)]\tLoss: 4.058125, Accu: 12.50%\n",
      "Train Epoch: 1 [9200/9990 (92%)]\tLoss: 4.675410, Accu: 0.00%\n",
      "Train Epoch: 1 [9280/9990 (93%)]\tLoss: 4.113190, Accu: 25.00%\n",
      "Train Epoch: 1 [9360/9990 (94%)]\tLoss: 4.643609, Accu: 0.00%\n",
      "Train Epoch: 1 [9440/9990 (94%)]\tLoss: 4.605283, Accu: 0.00%\n",
      "Train Epoch: 1 [9520/9990 (95%)]\tLoss: 4.319846, Accu: 0.00%\n",
      "Train Epoch: 1 [9600/9990 (96%)]\tLoss: 4.794664, Accu: 0.00%\n",
      "Train Epoch: 1 [9680/9990 (97%)]\tLoss: 4.607998, Accu: 0.00%\n",
      "Train Epoch: 1 [9760/9990 (98%)]\tLoss: 3.958345, Accu: 12.50%\n",
      "Train Epoch: 1 [9840/9990 (98%)]\tLoss: 4.595370, Accu: 0.00%\n",
      "Train Epoch: 1 [9920/9990 (99%)]\tLoss: 4.900001, Accu: 0.00%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/projectx/Documents/GitHub repos/Video-Classification/functions.py\", line 62, in __getitem__\n    y = torch.LongTensor([self.labels[index]])                             # (labels) LongTensor are for int64 instead of FloatTensor\nValueError: too many dimensions 'str'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-71b533e5442c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# train, test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mepoch_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_test_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a521ccf5cbeb>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(model, device, optimizer, test_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mall_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;31m# distribute data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/projectx/Documents/GitHub repos/Video-Classification/functions.py\", line 62, in __getitem__\n    y = torch.LongTensor([self.labels[index]])                             # (labels) LongTensor are for int64 instead of FloatTensor\nValueError: too many dimensions 'str'\n"
     ]
    }
   ],
   "source": [
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, cnn3d, device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation(cnn3d, device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    B = np.array(epoch_train_scores)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    D = np.array(epoch_test_scores)\n",
    "    np.save('./3DCNN_epoch_training_losses.npy', A)\n",
    "    np.save('./3DCNN_epoch_training_scores.npy', B)\n",
    "    np.save('./3DCNN_epoch_test_loss.npy', C)\n",
    "    np.save('./3DCNN_epoch_test_score.npy', D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
